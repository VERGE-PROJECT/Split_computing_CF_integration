{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0b84d1-8be7-4b8d-9c73-07412006b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cogflow as cf\n",
    "web_downloader_op=cf.load_component(url='https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml')\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3e7f3e-903b-496f-bc03-c607ce830edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Using cached pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from pytorch_lightning) (2.4.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.11/site-packages (from pytorch_lightning) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.11/site-packages (from pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.9.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Using cached torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from pytorch_lightning) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.11/site-packages (from pytorch_lightning) (4.12.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
      "  Using cached lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.10.9)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.1.0->pytorch_lightning) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch_lightning) (12.6.77)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.24.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n",
      "Using cached pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "Using cached lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
      "Using cached torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.11.8 pytorch_lightning-2.4.0 torchmetrics-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc8e341-9faa-496b-9ea7-31ff32cfb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.softmax(self.l3(a), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n",
    "        \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "class SplitComputingAgent(cf.pyfunc.PythonModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            action_dim,\n",
    "            max_action,\n",
    "            discount=0.99,\n",
    "            tau=0.005,\n",
    "            policy_noise=0.2,\n",
    "            noise_clip=0.5,\n",
    "            policy_freq=2\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        self.total_it += 1\n",
    "        \n",
    "        # Sample replay buffer\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target_q = reward + not_done * self.discount * target_q\n",
    "\n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.select_action(model_input)\n",
    "\n",
    "    def save(self, filename):\n",
    "        critic_filename = filename + \"_critic\"\n",
    "        critic_optimizer_filename = filename + \"_critic_optimizer\"\n",
    "        actor_filename = filename + \"_actor\"\n",
    "        actor_optimizer_filename = filename + \"_actor_optimizer\"\n",
    "        \n",
    "        cf.pytorch.save_state_dict(self.critic.state_dict(), critic_filename)\n",
    "        cf.pytorch.save_state_dict(self.critic_optimizer.state_dict(), critic_optimizer_filename)\n",
    "        cf.pytorch.save_state_dict(self.actor.state_dict(), actor_filename)\n",
    "        cf.pytorch.save_state_dict(self.actor_optimizer.state_dict(), actor_optimizer_filename)\n",
    "        \n",
    "        return {\n",
    "            \"criticFileName\": critic_filename,\n",
    "            \"criticOptimizerFileName\": critic_optimizer_filename,\n",
    "            \"actorFileName\": actor_filename,\n",
    "            \"actorOptimizerFileName\": actor_optimizer_filename\n",
    "        }\n",
    "\n",
    "    def load(self, filename):\n",
    "        critic_filename = filename + \"_critic\"\n",
    "        critic_optimizer_filename = filename + \"_critic_optimizer\"\n",
    "        actor_filename = filename + \"_actor\"\n",
    "        actor_optimizer_filename = filename + \"_actor_optimizer\"\n",
    "        \n",
    "        self.critic.load_state_dict(torch.load(critic_filename))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(critic_optimizer_filename))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        \n",
    "        self.actor.load_state_dict(torch.load(actor_filename))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(actor_optimizer_filename))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        \n",
    "# UE Type Enumeration\n",
    "class UEType(Enum):\n",
    "    SMARTPHONE = 1\n",
    "    TABLET = 2\n",
    "    LAPTOP = 3\n",
    "    IOT = 4\n",
    "\n",
    "# Wireless Channel Classes\n",
    "class WirelessLink:\n",
    "    def __init__(self, bandwidth):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.rsrp = None\n",
    "        self.rsrq = None\n",
    "        self.sinr = None\n",
    "        self.cqi = None\n",
    "\n",
    "    def update_channel_conditions(self, rsrp, rsrq, sinr, cqi):\n",
    "        self.rsrp = rsrp\n",
    "        self.rsrq = rsrq\n",
    "        self.sinr = sinr\n",
    "        self.cqi = cqi\n",
    "\n",
    "    def data_rate(self):\n",
    "        if self.sinr is None:\n",
    "            raise ValueError(\"Channel conditions not set\")\n",
    "        return self.bandwidth * np.log2(1 + np.power(10,(self.sinr / 10)))\n",
    "\n",
    "    def latency(self, data_size):\n",
    "        return data_size / self.data_rate()\n",
    "\n",
    "class WirelessChannel:\n",
    "    def __init__(self, channel_file):\n",
    "        self.channel_data = pd.read_csv(channel_file)\n",
    "        self.channel_data['timestamp'] = pd.to_datetime(self.channel_data['timestamp'])\n",
    "        self.timestamps = sorted(self.channel_data['timestamp'].unique())\n",
    "        self.current_step = 0\n",
    "\n",
    "    def get_channel_conditions(self, ue_id, step):\n",
    "        if step >= len(self.timestamps):\n",
    "            raise ValueError(f\"Step {step} is out of range. Max step is {len(self.timestamps) - 1}\")\n",
    "\n",
    "        timestamp = self.timestamps[step]\n",
    "        conditions = self.channel_data[\n",
    "            (self.channel_data['UE_ID'] == ue_id) &\n",
    "            (self.channel_data['timestamp'] == timestamp)\n",
    "        ].iloc[0]\n",
    "\n",
    "        return {\n",
    "            'rsrp': conditions['RSRP'],\n",
    "            'rsrq': conditions['RSRQ'],\n",
    "            'sinr': conditions['SINR'],\n",
    "            'cqi': conditions['CQI']\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.timestamps):\n",
    "            self.current_step = 0\n",
    "\n",
    "# MEC Server Class\n",
    "class MECServer:\n",
    "    def __init__(self, cpu, mem, gpu):\n",
    "        self.total_cpu = cpu  # in MIPS\n",
    "        self.total_mem = mem  # in GB\n",
    "        self.total_gpu = gpu  # in FLOPS\n",
    "\n",
    "        self.available_cpu = cpu\n",
    "        self.available_mem = mem\n",
    "        self.available_gpu = gpu\n",
    "\n",
    "        self.tasks = []\n",
    "\n",
    "    def can_accept_task(self, mem_req):\n",
    "        return self.available_mem >= mem_req\n",
    "\n",
    "    def get_utilization(self):\n",
    "        return {\n",
    "            'cpu': (self.total_cpu - self.available_cpu) / self.total_cpu,\n",
    "            'mem': (self.total_mem - self.available_mem) / self.total_mem,\n",
    "            'gpu': (self.total_gpu - self.available_gpu) / self.total_gpu\n",
    "        }\n",
    "\n",
    "    def process_task(self, cpu_demand, gpu_demand, memory_demand):\n",
    "        if self.can_accept_task(memory_demand):\n",
    "            processing_time = min(cpu_demand / self.available_cpu,\n",
    "                                gpu_demand / self.available_gpu)\n",
    "            return processing_time\n",
    "        else:\n",
    "            return float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.available_cpu = self.total_cpu\n",
    "        self.available_mem = self.total_mem\n",
    "        self.available_gpu = self.total_gpu\n",
    "        self.tasks = []\n",
    "\n",
    "# DNN Task Class (simplified version)\n",
    "class DNNTask:\n",
    "    def __init__(self, num_layers=4):\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_demands = self._generate_layer_demands()\n",
    "\n",
    "    def _generate_layer_demands(self):\n",
    "        return [{\n",
    "            'local_cpu_demand': 1e9 * (i + 1),\n",
    "            'local_gpu_demand': 2e9 * (i + 1),\n",
    "            'local_memory_demand': 1e8 * (i + 1),\n",
    "            'remote_cpu_demand': 5e8 * (i + 1),\n",
    "            'remote_gpu_demand': 1e9 * (i + 1),\n",
    "            'remote_memory_demand': 5e7 * (i + 1),\n",
    "            'transmision_data_demand': 1e6 * (i + 1)\n",
    "        } for i in range(self.num_layers)]\n",
    "\n",
    "    def get_split_info(self, split_point):\n",
    "        if split_point < 0 or split_point > self.num_layers:\n",
    "            raise ValueError(f\"Invalid split point: {split_point}\")\n",
    "\n",
    "        local_layers = self.layer_demands[:split_point]\n",
    "        remote_layers = self.layer_demands[split_point:]\n",
    "\n",
    "        return {\n",
    "            'local_cpu_demand': sum(layer['local_cpu_demand'] for layer in local_layers),\n",
    "            'local_gpu_demand': sum(layer['local_gpu_demand'] for layer in local_layers),\n",
    "            'local_memory_demand': sum(layer['local_memory_demand'] for layer in local_layers),\n",
    "            'remote_cpu_demand': sum(layer['remote_cpu_demand'] for layer in remote_layers),\n",
    "            'remote_gpu_demand': sum(layer['remote_gpu_demand'] for layer in remote_layers),\n",
    "            'remote_memory_demand': sum(layer['remote_memory_demand'] for layer in remote_layers),\n",
    "            'transmision_data_demand': sum(layer['transmision_data_demand'] for layer in remote_layers)\n",
    "        }\n",
    "\n",
    "# UE Class\n",
    "class UE:\n",
    "    DEFAULT_RESOURCES = {\n",
    "        UEType.SMARTPHONE: {\n",
    "            \"cpu\": 2.0e9, \"gpu\": 50e9, \"mem\": 4e9, \"bat\": 3000,\n",
    "            \"e_cpu\": 1e-9, \"e_gpu\": 1e-12, \"e_mem\": 1e-6,\n",
    "            \"p_base\": 0.1, \"p_tx\": 0.5\n",
    "        },\n",
    "        UEType.TABLET: {\n",
    "            \"cpu\": 2.5e9, \"gpu\": 7e9, \"mem\": 8e9, \"bat\": 7000,\n",
    "            \"e_cpu\": 9e-10, \"e_gpu\": 9e-13, \"e_mem\": 9e-7,\n",
    "            \"p_base\": 0.15, \"p_tx\": 0.6\n",
    "        },\n",
    "        UEType.LAPTOP: {\n",
    "            \"cpu\": 3.5e9, \"gpu\": 2e12, \"mem\": 16e9, \"bat\": 5000,\n",
    "            \"e_cpu\": 8e-10, \"e_gpu\": 8e-13, \"e_mem\": 8e-7,\n",
    "            \"p_base\": 0.2, \"p_tx\": 0.7\n",
    "        },\n",
    "        UEType.IOT: {\n",
    "            \"cpu\": 1.0e9, \"gpu\": 0, \"mem\": 1e9, \"bat\": 1000,\n",
    "            \"e_cpu\": 1.2e-9, \"e_gpu\": 0, \"e_mem\": 1.2e-6,\n",
    "            \"p_base\": 0.05, \"p_tx\": 0.3\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, ue_id, ue_type, num_layers=4):\n",
    "        self.ue_id = ue_id\n",
    "        self.ue_type = ue_type\n",
    "        self.resources = self.DEFAULT_RESOURCES[ue_type].copy()\n",
    "        self.cpu_load = 20\n",
    "        self.gpu_load = 10 if self.resources['gpu'] > 0 else 0\n",
    "        self.mem_load = 10\n",
    "        self.bat_level = self.resources['bat']\n",
    "        self.wireless_link = WirelessLink(10e6)\n",
    "        self.task = DNNTask(num_layers=num_layers)\n",
    "        self.time = 0\n",
    "\n",
    "    def update(self):\n",
    "        self.time += 1\n",
    "        cpu_change = random.normalvariate(0, 0.05 * self.resources['cpu'])\n",
    "        self.cpu_load = max(0, min(self.resources['cpu'], self.cpu_load + cpu_change))\n",
    "\n",
    "        if self.resources['gpu'] > 0:\n",
    "            gpu_change = random.normalvariate(0, 0.07 * self.resources['gpu'])\n",
    "            self.gpu_load = max(0, min(self.resources['gpu'], self.gpu_load + gpu_change))\n",
    "\n",
    "        mem_change = random.normalvariate(0, 0.07 * self.resources['mem'])\n",
    "        self.mem_load = max(0, min(self.resources['mem'], self.mem_load + mem_change))\n",
    "\n",
    "        energy_consumption = (self.cpu_load * self.resources['e_cpu'] + \n",
    "                            self.gpu_load * self.resources['e_gpu'] + \n",
    "                            self.mem_load * self.resources['e_mem'] + \n",
    "                            self.resources['p_base']) / 3600\n",
    "        self.bat_level -= energy_consumption\n",
    "\n",
    "        if random.random() < 0.1 or self.bat_level < 0.1 * self.resources['bat']:\n",
    "            self.bat_level = self.resources['bat']\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'cpu_load': self.cpu_load / self.resources['cpu'],\n",
    "            'gpu_load': self.gpu_load / self.resources['gpu'] if self.resources['gpu'] > 0 else 0,\n",
    "            'mem_load': self.mem_load / self.resources['mem'],\n",
    "            'bat_level': self.bat_level / self.resources['bat'],\n",
    "            'rsrp': self.wireless_link.rsrp/100,\n",
    "            'rsrq': self.wireless_link.rsrq/20,\n",
    "            'sinr': self.wireless_link.sinr/10\n",
    "        }\n",
    "\n",
    "    def compute_local(self, cpu_demand, gpu_demand, memory_demand):\n",
    "        available_cpu = self.resources['cpu'] - self.cpu_load\n",
    "        available_gpu = self.resources['gpu'] - self.gpu_load\n",
    "        available_mem = self.resources['mem'] - self.mem_load\n",
    "\n",
    "        if available_mem < memory_demand:\n",
    "            return float('inf')\n",
    "\n",
    "        cpu_time = cpu_demand / available_cpu if available_cpu > 0 else float('inf')\n",
    "        gpu_time = gpu_demand / available_gpu if available_gpu > 0 else float('inf')\n",
    "        return min(cpu_time, gpu_time)\n",
    "\n",
    "    def compute_communication(self, data_size):\n",
    "        return self.wireless_link.latency(data_size/80)\n",
    "\n",
    "    def calculate_energy_consumption(self, cpu_usage, gpu_usage, mem_usage, duration):\n",
    "        return (cpu_usage * self.resources['e_cpu'] + \n",
    "                gpu_usage * self.resources['e_gpu'] + \n",
    "                mem_usage * self.resources['e_mem'] + \n",
    "                self.resources['p_base']) * duration\n",
    "\n",
    "# Split Computing Environment\n",
    "class SplitComputingEnv:\n",
    "    def __init__(self, channel_file, num_ues):\n",
    "        super(SplitComputingEnv, self).__init__()\n",
    "        self.channel_data = pd.read_csv(channel_file)\n",
    "        distinct_ue_ids = self.channel_data['UE_ID'].nunique()\n",
    "        self.max_steps = self.channel_data['timestamp'].nunique()\n",
    "\n",
    "        if num_ues > distinct_ue_ids:\n",
    "            raise ValueError(f\"Requested number of UEs ({num_ues}) exceeds the number of distinct UE IDs in the channel file ({distinct_ue_ids})\")\n",
    "\n",
    "        self.num_ues = num_ues\n",
    "        self.wireless_channel = WirelessChannel(channel_file)\n",
    "        self.action_space = 5\n",
    "        self.observation_space = 7\n",
    "        self.actual_ue_ids = self.channel_data['UE_ID'].unique()[:num_ues]\n",
    "\n",
    "        ue_type_probabilities = [0.5, 0.15, 0.25, 0.1]\n",
    "        ue_types = np.random.choice(list(UEType), size=num_ues, p=ue_type_probabilities)\n",
    "        self.ues = {ue_id: UE(int(ue_id), ue_type, num_layers=self.action_space) \n",
    "                   for ue_id, ue_type in zip(self.actual_ue_ids, ue_types)}\n",
    "\n",
    "        self.mec_server = MECServer(cpu=100e9, mem=256e9, gpu=1e12)\n",
    "        self.current_step = 0\n",
    "        self.ues_processed_this_step = set()\n",
    "        self.total_energy_consumed = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.total_time_taken = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.ue_data_rate = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.sla_violation = {ue_id: 0 for ue_id in self.actual_ue_ids}\n",
    "\n",
    "    def step(self, action, ue_id):\n",
    "        if ue_id not in self.ues:\n",
    "            raise ValueError(f\"Invalid UE ID: {ue_id}\")\n",
    "        energy_consumption = 0\n",
    "        ue = self.ues[ue_id]\n",
    "        ue.update()\n",
    "        channel_conditions = self.wireless_channel.get_channel_conditions(ue_id, self.current_step)\n",
    "        ue.wireless_link.update_channel_conditions(**channel_conditions)\n",
    "\n",
    "        split_info = ue.task.get_split_info(action)\n",
    "        local_time = ue.compute_local(split_info['local_cpu_demand'], split_info['local_gpu_demand'],\n",
    "                                      split_info['local_memory_demand'])\n",
    "        comm_time = ue.compute_communication(split_info['transmision_data_demand'])\n",
    "\n",
    "        if self.mec_server.can_accept_task(split_info['remote_memory_demand']):\n",
    "            remote_time = self.mec_server.process_task(split_info['remote_cpu_demand'], split_info['remote_gpu_demand'],\n",
    "                                                       split_info['remote_memory_demand'])\n",
    "        else:\n",
    "            remote_time = float('inf')\n",
    "\n",
    "        total_time = local_time + comm_time + remote_time\n",
    "        if total_time <float('inf'):\n",
    "            energy_consumption = ue.calculate_energy_consumption(\n",
    "                split_info['local_cpu_demand'],\n",
    "                split_info['local_gpu_demand'],\n",
    "                split_info['local_memory_demand'],\n",
    "                local_time\n",
    "            ) + ue.resources['p_tx'] * comm_time\n",
    "            self.total_energy_consumed[ue_id].append(energy_consumption)\n",
    "            self.total_time_taken[ue_id].append(total_time)\n",
    "            self.ue_data_rate[ue_id].append(ue.wireless_link.data_rate())\n",
    "            reward = self.compute_reward(total_time, energy_consumption, ue.bat_level / ue.resources['bat'])\n",
    "        else:\n",
    "            self.sla_violation[ue_id] += 1\n",
    "            reward = -1\n",
    "\n",
    "\n",
    "        self.ues_processed_this_step.add(ue_id)\n",
    "\n",
    "        # Check if all UEs have been processed for this step\n",
    "        if len(self.ues_processed_this_step) == self.num_ues:\n",
    "            self.current_step += 1\n",
    "            self.ues_processed_this_step.clear()\n",
    "\n",
    "        done = self.current_step >= self.max_steps\n",
    "        info = {\n",
    "            'energy_consumption': energy_consumption,\n",
    "            'total_time': total_time\n",
    "        }\n",
    "        return ue.get_state(), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.ues_processed_this_step.clear()\n",
    "        self.total_energy_consumed = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.total_time_taken = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.ue_data_rate = {ue_id: [] for ue_id in self.actual_ue_ids}\n",
    "        self.sla_violation = {ue_id: 0 for ue_id in self.actual_ue_ids}\n",
    "        # Reset UEs and update their initial channel conditions\n",
    "        initial_states = {}\n",
    "        for ue_id, ue in self.ues.items():\n",
    "            ue.__init__(ue.ue_id, ue.ue_type)  # Reset UE to initial state\n",
    "\n",
    "            # Get initial channel conditions for this UE\n",
    "            initial_conditions = self.wireless_channel.get_channel_conditions(ue_id, self.current_step)\n",
    "\n",
    "            # Update UE's wireless link with initial conditions\n",
    "            ue.wireless_link.update_channel_conditions(**initial_conditions)\n",
    "\n",
    "            # Get the initial state for this UE\n",
    "            initial_states[ue_id] = ue.get_state()\n",
    "\n",
    "        self.mec_server.reset()\n",
    "        self.current_step += 1\n",
    "        return initial_states\n",
    "\n",
    "    def compute_reward(self, delay, energy, battery_level):\n",
    "        # before computing the reward we normalize the delay and energy:\n",
    "        delay = delay*10\n",
    "        energy = energy / 10\n",
    "        w1, w2, w3 = 0.4, 0.4, 0.2\n",
    "        r_time = 1 / (delay + 1e-6)\n",
    "        r_energy = 1 / (energy + 1e-6)\n",
    "        r_battery = battery_level\n",
    "        return w1 * r_time + w2 * r_energy + w3 * r_battery\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_average_energy_consumption(self):\n",
    "        return {ue_id: np.array(self.total_energy_consumed[ue_id]).mean() for ue_id in self.total_energy_consumed.keys()}\n",
    "\n",
    "    def get_average_total_time(self):\n",
    "        return {ue_id: (np.array(self.total_time_taken[ue_id])*100).mean() for ue_id in self.total_time_taken.keys()}\n",
    "\n",
    "    def get_total_sla_violation(self):\n",
    "        return {ue_id: self.sla_violation[ue_id] for ue_id in self.sla_violation.items()}\n",
    "\n",
    "    def get_total_data_rate(self):\n",
    "        return  self.ue_data_rate\n",
    "    def get_overall_average_energy_consumption(self):\n",
    "\n",
    "        means = [np.mean(energy) for energy in self.total_energy_consumed.values() if len(energy) > 0]\n",
    "        result = np.mean(means) if means else 0\n",
    "        return result\n",
    "\n",
    "    def get_overall_average_total_time(self):\n",
    "\n",
    "        means = [np.mean(time) *100 for time in self.total_time_taken.values() if len(time) > 0]\n",
    "        result = np.mean(means) if means else 0\n",
    "        return result\n",
    "\n",
    "    def get_overall_ue_data_rate(self):\n",
    "\n",
    "        means = [np.mean(rate)/1e6 for rate in self.ue_data_rate.values() if len(rate) > 0]\n",
    "        result = np.mean(means) if means else 0\n",
    "        return result\n",
    "\n",
    "    def get_overall_average_sla_violation(self):\n",
    "        violations = [arr for arr in self.sla_violation.values()]\n",
    "        return np.mean(violations)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bbe7c8-3933-4027-8715-414d4d35dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split_computing(\n",
    "    num_ues=3,\n",
    "    channel_file=\"channel_data.csv\",\n",
    "    seed=0,\n",
    "    start_timesteps=25000,\n",
    "    eval_freq=5000,\n",
    "    max_timesteps=1000000,\n",
    "    expl_noise=0.1,\n",
    "    batch_size=256,\n",
    "    discount=0.99,\n",
    "    tau=0.005,\n",
    "    policy_noise=0.2,\n",
    "    noise_clip=0.5,\n",
    "    policy_freq=2\n",
    "):\n",
    "    # Set up cogflow experiment\n",
    "    experiment_id = cf.set_experiment(\n",
    "        experiment_name=\"Split Computing DRL\",\n",
    "    )\n",
    "    cf.pytorch.autolog()\n",
    "\n",
    "    # Create environment and agent\n",
    "    env = SplitComputingEnv(channel_file, num_ues=num_ues)\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space\n",
    "    action_dim = env.action_space\n",
    "    max_action = env.action_space - 1  # Maximum split point\n",
    "    \n",
    "    agent = SplitComputingAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=max_action,\n",
    "        discount=discount,\n",
    "        tau=tau,\n",
    "        policy_noise=policy_noise,\n",
    "        noise_clip=noise_clip,\n",
    "        policy_freq=policy_freq\n",
    "    )\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(1e6)\n",
    "\n",
    "    with cf.start_run(run_name=\"split_computing_training\") as run:\n",
    "        # Log parameters\n",
    "        cf.log_param(\"num_ues\", num_ues)\n",
    "        cf.log_param(\"seed\", seed)\n",
    "        cf.log_param(\"batch_size\", batch_size)\n",
    "        cf.log_param(\"discount\", discount)\n",
    "        cf.log_param(\"start_timesteps\", start_timesteps)\n",
    "        \n",
    "        # Training loop\n",
    "        states = env.reset()\n",
    "        episode_num = 0\n",
    "        \n",
    "        for t in tqdm(range(int(max_timesteps)), desc=\"Training Progress\"):\n",
    "            episode_rewards = {ue_id: 0 for ue_id in env.actual_ue_ids}\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                for ue_id in env.actual_ue_ids:\n",
    "                    state = states[ue_id]\n",
    "                    \n",
    "                    # Select action\n",
    "                    if t < start_timesteps:\n",
    "                        action = np.random.randint(0, env.action_space)\n",
    "                    else:\n",
    "                        action = agent.select_action(np.array(list(d.values())))\n",
    "                        action = np.clip(\n",
    "                            action + np.random.normal(0, max_action * expl_noise),\n",
    "                            0,\n",
    "                            max_action\n",
    "                        )\n",
    "                    # Execute action\n",
    "                    next_state, reward, done, info = env.step(int(action), ue_id)\n",
    "                    \n",
    "                    # Store transition\n",
    "                    replay_buffer.add(state, action, next_state, reward, float(done))\n",
    "                    \n",
    "                    episode_rewards[ue_id] += reward\n",
    "                    states[ue_id] = next_state\n",
    "                    \n",
    "                    # Train agent\n",
    "                    if t >= start_timesteps:\n",
    "                        agent.train(replay_buffer, batch_size)\n",
    "            \n",
    "            # Log metrics\n",
    "            avg_energy = env.get_overall_average_energy_consumption()\n",
    "            avg_time = env.get_overall_average_total_time()\n",
    "            avg_data_rate = env.get_overall_ue_data_rate()\n",
    "            avg_violations = env.get_overall_average_sla_violation()\n",
    "            \n",
    "            cf.log_metric(\"average_energy\", avg_energy, step=episode_num)\n",
    "            cf.log_metric(\"average_time\", avg_time, step=episode_num)\n",
    "            cf.log_metric(\"average_data_rate\", avg_data_rate, step=episode_num)\n",
    "            cf.log_metric(\"sla_violations\", avg_violations, step=episode_num)\n",
    "            \n",
    "            # Save model periodically\n",
    "            #if (t + 1) % eval_freq == 0:\n",
    "            if True:\n",
    "                model_artifacts = agent.save(f\"split_computing_{t+1}\")\n",
    "                # Log model\n",
    "                model_info = cf.pyfunc.log_model(\n",
    "                    artifact_path=f\"split_computing_{t+1}\",\n",
    "                    python_model=agent,\n",
    "                    artifacts=model_artifacts,\n",
    "                    pip_requirements=[],\n",
    "                    input_example=state,\n",
    "                    signature=cf.models.infer_signature(np.array(list(state.values())).reshape(1,-1), np.array([action]))\n",
    "                )   \n",
    "            episode_num += 1\n",
    "            states = env.reset()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0a0738-2bcd-4ca7-b9d6-3ab39ef2e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3gpp_channel_data(num_ues=5, duration_minutes=1000, sampling_rate_ms=100):\n",
    "    \"\"\"\n",
    "    Generate channel data following 3GPP specifications for urban macro cell scenarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_ues : int\n",
    "        Number of UEs to simulate\n",
    "    duration_minutes : int\n",
    "        Duration of the simulation in minutes\n",
    "    sampling_rate_ms : int\n",
    "        Sampling rate in milliseconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the generated CSV file\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    Following 3GPP TR 38.901 for channel modeling:\n",
    "    - Path loss model: Urban Macro (UMa)\n",
    "    - Frequency: 2GHz\n",
    "    - BS height: 25m\n",
    "    - UE height: 1.5m\n",
    "    - Distance range: 10-500m\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_path_loss(distance, f_c=2.0):\n",
    "        \"\"\"Calculate path loss based on 3GPP UMa model\"\"\"\n",
    "        # 3GPP TR 38.901 Urban Macro path loss model\n",
    "        h_BS = 25  # Base station height in meters\n",
    "        h_UT = 1.5  # User terminal height in meters\n",
    "        \n",
    "        # Calculate break point distance\n",
    "        h_E = 1  # Effective environment height\n",
    "        h_BP = 4 * (h_BS - h_E) * (h_UT - h_E) * f_c / 0.3\n",
    "        \n",
    "        if distance < h_BP:\n",
    "            # LOS path loss before break point\n",
    "            PL = 28.0 + 22*np.log10(distance) + 20*np.log10(f_c)\n",
    "        else:\n",
    "            # LOS path loss after break point\n",
    "            PL = 28.0 + 40*np.log10(distance) + 20*np.log10(f_c) - 9*np.log10(h_BP**2 + (h_BS-h_UT)**2)\n",
    "        \n",
    "        return PL\n",
    "\n",
    "    def generate_shadow_fading(size):\n",
    "        \"\"\"Generate shadow fading with log-normal distribution\"\"\"\n",
    "        # 3GPP specifies 4dB standard deviation for UMa LOS\n",
    "        return np.random.normal(0, 4, size)\n",
    "    \n",
    "    def calculate_sinr(path_loss, shadow_fading, interference_power=-90):\n",
    "        \"\"\"Calculate SINR based on path loss and interference\"\"\"\n",
    "        tx_power = 43  # BS transmission power in dBm (typical macro cell)\n",
    "        noise_floor = -174 + 10*np.log10(20e6)  # Thermal noise for 20MHz bandwidth\n",
    "        \n",
    "        # Received power = Tx power - path loss + shadow fading\n",
    "        rx_power = tx_power - path_loss + shadow_fading\n",
    "        \n",
    "        # SINR calculation\n",
    "        interference_plus_noise = 10*np.log10(10**(interference_power/10) + 10**(noise_floor/10))\n",
    "        sinr = rx_power - interference_plus_noise\n",
    "        \n",
    "        return sinr\n",
    "    \n",
    "    def map_sinr_to_cqi(sinr):\n",
    "        \"\"\"Map SINR to CQI according to 3GPP specifications\"\"\"\n",
    "        # Simplified CQI mapping based on SINR ranges\n",
    "        cqi_ranges = [\n",
    "            (-float('inf'), -6.9),\n",
    "            (-6.9, -5.1), (-5.1, -3.3), (-3.3, -1.5), (-1.5, 0.3),\n",
    "            (0.3, 2.1), (2.1, 3.9), (3.9, 5.7), (5.7, 7.5),\n",
    "            (7.5, 9.3), (9.3, 11.1), (11.1, 12.9), (12.9, 14.7),\n",
    "            (14.7, 16.5), (16.5, float('inf'))\n",
    "        ]\n",
    "        \n",
    "        for cqi, (min_sinr, max_sinr) in enumerate(cqi_ranges):\n",
    "            if min_sinr <= sinr < max_sinr:\n",
    "                return cqi + 1\n",
    "        return 15\n",
    "    \n",
    "    def calculate_rsrp(path_loss, shadow_fading):\n",
    "        \"\"\"Calculate RSRP based on path loss\"\"\"\n",
    "        tx_power = 43  # BS transmission power in dBm\n",
    "        return tx_power - path_loss + shadow_fading\n",
    "    \n",
    "    def calculate_rsrq(rsrp, num_resource_blocks=100):\n",
    "        \"\"\"Calculate RSRQ based on RSRP and RSSI\"\"\"\n",
    "        # Simplified RSRQ calculation\n",
    "        noise_per_rb = -120  # Noise per resource block in dBm\n",
    "        rssi = 10*np.log10(10**(rsrp/10) + num_resource_blocks * 10**(noise_per_rb/10))\n",
    "        rsrq = rsrp - rssi + 10*np.log10(num_resource_blocks)\n",
    "        return np.clip(rsrq, -19.5, -3)\n",
    "\n",
    "    # Calculate number of samples\n",
    "    num_samples = int((duration_minutes * 60 * 1000) / sampling_rate_ms)\n",
    "    timestamps = [datetime.now() + timedelta(milliseconds=i*sampling_rate_ms) for i in range(num_samples)]\n",
    "    \n",
    "    # Generate UE distances (assuming some mobility)\n",
    "    ue_distances = {ue_id: [] for ue_id in range(num_ues)}\n",
    "    for ue_id in range(num_ues):\n",
    "        # Initial distance\n",
    "        current_distance = np.random.uniform(10, 500)\n",
    "        # Random walk for distance changes\n",
    "        for _ in range(num_samples):\n",
    "            # Add some random movement (-2 to +2 meters per sample)\n",
    "            current_distance += np.random.uniform(-2, 2)\n",
    "            current_distance = np.clip(current_distance, 10, 500)  # Keep within valid range\n",
    "            ue_distances[ue_id].append(current_distance)\n",
    "    \n",
    "    # Generate data for each UE and timestamp\n",
    "    data = []\n",
    "    for t_idx, timestamp in enumerate(timestamps):\n",
    "        for ue_id in range(num_ues):\n",
    "            distance = ue_distances[ue_id][t_idx]\n",
    "            path_loss = calculate_path_loss(distance)\n",
    "            shadow_fading = generate_shadow_fading(1)[0]\n",
    "            \n",
    "            # Calculate channel conditions\n",
    "            rsrp = calculate_rsrp(path_loss, shadow_fading)\n",
    "            rsrq = calculate_rsrq(rsrp)\n",
    "            sinr = calculate_sinr(path_loss, shadow_fading)\n",
    "            cqi = map_sinr_to_cqi(sinr)\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'UE_ID': ue_id,\n",
    "                'RSRP': round(rsrp, 2),\n",
    "                'RSRQ': round(rsrq, 2),\n",
    "                'SINR': round(sinr, 2),\n",
    "                'CQI': int(cqi),\n",
    "                'distance': round(distance, 2)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = 'channel_data_3gpp.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Channel Data Statistics ===\")\n",
    "    print(f\"Number of UEs: {num_ues}\")\n",
    "    print(f\"Duration: {duration_minutes} minutes\")\n",
    "    print(f\"Sampling rate: {sampling_rate_ms}ms\")\n",
    "    print(\"\\nMetrics ranges:\")\n",
    "    for metric in ['RSRP', 'RSRQ', 'SINR', 'CQI']:\n",
    "        print(f\"{metric}: {df[metric].min():.2f} to {df[metric].max():.2f}\")\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9607a50-253b-472c-88d7-7144b71d1fcd",
   "metadata": {},
   "source": [
    "# Create channel data file\n",
    "channel_file = generate_3gpp_channel_data(\n",
    "    num_ues=3,                # Number of UEs\n",
    "    duration_minutes=1000,    # Duration of simulation\n",
    "    sampling_rate_ms=100      # Sampling rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb248a7-3db6-4092-a1ab-df0438ddd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/05 13:31:39 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "Training Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48927527  0.01269501  0.35003099  0.99600554 -0.5198     -0.15\n",
      "  3.768     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1/1 [07:18<00:00, 438.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST request successful\n",
      "POST request successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "channel_file = 'channel_data_3gpp.csv'\n",
    "trained_agent = train_split_computing(\n",
    "    num_ues=3,\n",
    "    channel_file=channel_file,\n",
    "    max_timesteps=1,  # Reduced for demonstration\n",
    "    eval_freq=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4777ca11-e071-448b-a10f-9c8516a4f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mlflow/0/1f9954c2dddf4dfca38206389f9abca7/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "models = cf.search_registered_models()\n",
    "latest_model = max(models, key=lambda m: m.creation_timestamp)\n",
    "print(latest_model.latest_versions[0].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9516497-7d10-4ff3-81d8-423daa5b9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest model\n",
    "modeluri=\"s3://mlflow/7/cb3a276fd9c74a84a4a18a8dddf45155/artifacts/split_computing_1\"\n",
    "loaded_model=cf.pyfunc.load_model(modeluri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce077379-0ba2-4c6b-82d3-bfbd8cf63590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mlflow/0/1f9954c2dddf4dfca38206389f9abca7/artifacts/model\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Model does not have the \"python_function\" flavor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(latest_model\u001b[38;5;241m.\u001b[39mlatest_versions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msource)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cf\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39mload_model(latest_model\u001b[38;5;241m.\u001b[39mlatest_versions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m----> 7\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_latest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mload_latest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m latest_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(models, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mcreation_timestamp)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(latest_model\u001b[38;5;241m.\u001b[39mlatest_versions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_versions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/pyfunc/__init__.py:570\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m conf \u001b[38;5;241m=\u001b[39m model_meta\u001b[38;5;241m.\u001b[39mflavors\u001b[38;5;241m.\u001b[39mget(FLAVOR_NAME)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel does not have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAVOR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m flavor\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    572\u001b[0m         RESOURCE_DOES_NOT_EXIST,\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m model_py_version \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget(PY_VERSION)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m suppress_warnings:\n",
      "\u001b[0;31mMlflowException\u001b[0m: Model does not have the \"python_function\" flavor"
     ]
    }
   ],
   "source": [
    "def load_latest_model():\n",
    "    models = cf.search_registered_models()\n",
    "    latest_model = max(models, key=lambda m: m.creation_timestamp)\n",
    "    print(latest_model.latest_versions[0].source)\n",
    "    return cf.pyfunc.load_model(latest_model.latest_versions[0].source)\n",
    "loaded_model = load_latest_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "decb30f2-aa61-47e9-a868-a9619159c0ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Shape of input () does not match expected shape (-1, 7).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m test_state\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(test_state\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mtest_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted split point:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load and display training metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m, in \u001b[0;36mtest_inference\u001b[0;34m(model, test_state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_inference\u001b[39m(model, test_state):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Test the loaded model with sample input\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/pyfunc/__init__.py:412\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    410\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/models/utils.py:662\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pf_input, input_schema)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_actual_columns \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs):\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel inference is missing inputs. The model signature declares \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m inputs  but the provided value only has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m inputs. Note: the inputs were not named in the signature so we can \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly verify their count.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(input_schema\u001b[38;5;241m.\u001b[39minputs), num_actual_columns)\n\u001b[1;32m    659\u001b[0m         )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 662\u001b[0m     \u001b[43m_enforce_tensor_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mis_tensor_spec()\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m _enforce_col_schema(pf_input, input_schema)\n\u001b[1;32m    665\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/models/utils.py:585\u001b[0m, in \u001b[0;36m_enforce_tensor_schema\u001b[0;34m(pf_input, input_schema)\u001b[0m\n\u001b[1;32m    583\u001b[0m         new_pf_input \u001b[38;5;241m=\u001b[39m _enforce_tensor_spec(pf_input\u001b[38;5;241m.\u001b[39mto_numpy(), tensor_spec)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pf_input, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mor\u001b[39;00m _is_sparse_matrix(pf_input):\n\u001b[0;32m--> 585\u001b[0m     new_pf_input \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_tensor_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model contains a tensor-based model signature with no input names,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which suggests a numpy array input or a pandas dataframe input with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m proper column values, but an input of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pf_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    591\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m    592\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/models/utils.py:316\u001b[0m, in \u001b[0;36m_enforce_tensor_spec\u001b[0;34m(values, tensor_spec)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expected_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(actual_shape):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match expected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expected, actual \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(expected_shape, actual_shape):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expected \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mMlflowException\u001b[0m: Shape of input () does not match expected shape (-1, 7)."
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "def test_inference(model, test_state):\n",
    "    \"\"\"\n",
    "    Test the loaded model with sample input\n",
    "    \"\"\"\n",
    "    result = model.predict(test_state)\n",
    "    return result\n",
    "\n",
    "# Create test state\n",
    "test_state = {\n",
    "    'cpu_load': 0.7,\n",
    "    'gpu_load': 0.4,\n",
    "    'mem_load': 0.6,\n",
    "    'bat_level': 0.8,\n",
    "    'rsrp': -95,\n",
    "    'rsrq': -12,\n",
    "    'sinr': 15\n",
    "}\n",
    "\n",
    "test_state=np.array(test_state.values())\n",
    "# Run inference\n",
    "prediction = test_inference(loaded_model, test_state)\n",
    "print(\"Predicted split point:\", prediction)\n",
    "\n",
    "\n",
    "# Load and display training metrics\n",
    "def display_training_metrics():\n",
    "    runs = cf.search_runs()\n",
    "    latest_run = max(runs, key=lambda r: r.info.start_time)\n",
    "    \n",
    "    metrics = latest_run.data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d7879-2852-483e-b11b-2c844fd4422c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
